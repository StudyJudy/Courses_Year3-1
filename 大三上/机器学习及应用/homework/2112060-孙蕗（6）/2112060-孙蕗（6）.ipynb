{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8d3cb7",
   "metadata": {},
   "source": [
    "## 实验要求\n",
    "### 截止日期：12月15日\n",
    "作业的提交格式参考之前的说明，提交到18329300691@163.com\n",
    "### 基本要求\n",
    "a)\t基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "b)\t基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "### 中级要求\n",
    "a)  对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "b)\t对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "### 高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8575b",
   "metadata": {},
   "source": [
    "\n",
    "## 什么是决策树\n",
    "<img src=\"https://s2.loli.net/2022/10/23/yTpSLmgFWOh4Y5d.png\" style=\"zoom:66%\" align=\"left\"/>\n",
    "\n",
    "\n",
    "## 决策树的划分\n",
    "- 决策树主要分为三种：\n",
    "\tID3，C4.5和CART，它们分别对应的**特征选择准则**是信息增益（ID3），信息增益比（C4.5）和基尼指数（CART）。\n",
    "\t它们决定当前选择哪个特征进行数据划分，使得样本在当下能够被最大程度的划分。\n",
    "- 对于离散变量，选定**属性**分类即可；\n",
    "- 对于连续变量，需要选定**划分点**。\n",
    "- CART和C4.5支持数据特征为**连续分布**时的处理，能够完成对连续属性的离散化处理，主要通过二元切分的方式来处理连续型变量，这个分裂点的选择原则是使得划分后的子树中的“混乱程度”降低。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaeddc2",
   "metadata": {},
   "source": [
    "## C4.5算法\n",
    "- C4.5算法与ID3算法相似，其对ID3算法进行了改进。\n",
    "- 信息增益作为划分准则存在的问题：\n",
    "\n",
    "     信息增益偏向于选择取值较多的特征进行划分。⽐如学号这个特征，每个学生都有一个不同的学号，如果根据学号对样本进行分类，则每个学生都属于不同的类别，这样是没有意义的。而C4.5在生成过程中，用**信息增益比**来选择特征，可以校正这个问题。\n",
    "     \n",
    "- 特点\n",
    "  - 能够完成对连续属性的离散化处理\n",
    "  - 能够对不完整数据进行处理\n",
    "  - 需要对数据集进行多次的顺序扫描和排序\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0add97e",
   "metadata": {},
   "source": [
    "\n",
    "## CART算法\n",
    "- ID3和C4.5虽然在对训练样本集的学习中可以尽可能多的挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，就出现了根据**基尼指数**来选择的CART； \n",
    "- 对于给定的样本集合 ，其基尼指数为： $$ {Gini}(D)=1-\\sum_{k=1}^{K}\\left(\\frac{\\left|C_{k}\\right|}{|D|}\\right)^{2} $$\n",
    "   其中$𝐶_𝑘$是𝐷中属于第𝑘类的样本子集，K是类的个数。\n",
    "- 基尼系数的性质与信息熵一样：\n",
    "   度量随机变量的不确定度的大小；基尼指数越⼩表示数据的纯度越高，反之其值越大，样本集合的不确定性也就越大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d760b",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "- 决策树很容易出现**过拟合现象**。原因在于学习时完全考虑的是如何提⾼对训练数据的正确分类从⽽构建出过于复杂的决策树。\n",
    "- 解决这个问题的方法称为**剪枝**，即对已生成的树进行简化。具体地，就是从已生成的树上裁剪掉⼀些子树或叶节点，并将其根节点或父节点作为新的叶节点。 \n",
    "- 决策树的剪枝基本策略有**预剪枝 (Pre-Pruning)** 和 **后剪枝 (Post-Pruning)**\n",
    "   - **预剪枝**：是根据⼀些原则**极早的停止树增长**，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的幅度小于用户指定的幅度等。 \n",
    "   - **后剪枝**：是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点。是在生成决策树之后**自底向上**的对树中所有的非叶结点进⾏逐一考察 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17985d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff77560c",
   "metadata": {},
   "source": [
    "### 基本要求\n",
    "a)\t基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "\n",
    "b)\t基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89d310ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c86d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练集和测试集数据\n",
    "watermelon_train1_data = pd.read_csv(r\"C:/Users/LENOVO/Desktop/ML-6/Watermelon-train1.csv\", encoding='gbk')  \n",
    "watermelon_test1_data = pd.read_csv(r\"C:/Users/LENOVO/Desktop/ML-6/Watermelon-test1.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b4ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分类标签转换为数值表示，是->1，否->0\n",
    "watermelon_train1_data['好瓜'] = watermelon_train1_data['好瓜'].map({'是': 1, '否': 0})\n",
    "watermelon_test1_data['好瓜'] = watermelon_test1_data['好瓜'].map({'是': 1, '否': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac0acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对离散特征进行独热编码\n",
    "watermelon_train1_data = pd.get_dummies(watermelon_train1_data, columns=['色泽', '根蒂', '敲声', '纹理'])\n",
    "watermelon_test1_data = pd.get_dummies(watermelon_test1_data, columns=['色泽', '根蒂', '敲声', '纹理'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b0f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征和标签\n",
    "X_train = watermelon_train1_data.drop('好瓜', axis=1)\n",
    "y_train = watermelon_train1_data['好瓜']\n",
    "\n",
    "X_test = watermelon_test1_data.drop('好瓜', axis=1)\n",
    "y_test = watermelon_test1_data['好瓜']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec82be",
   "metadata": {},
   "source": [
    "\n",
    "## ID3算法\n",
    "- ID3算法的核⼼思想应用信息增益准则作为标准,介绍信息增益之前首先介绍一下信息熵和条件熵： \n",
    "- 熵（entropy）概念：\n",
    "\t    1948年，香农提出了“信息熵”的概念。在信息论与概率统计中，熵是表示随机变量不确定性的量。X是⼀个取值为有限个的离散随机变量，\n",
    "$$ H(X)=-\\sum_{i=1}^{n} p\\left(x_{i}\\right) \\log p\\left(x_{i}\\right)$$ \n",
    "$𝐻(𝑋)$就被称作随机变量𝑋的熵，它表示随机变量不确定的度量。熵取值越大，随机变量不确定性越大。当随机变量为均匀分布时，熵最大。当某一状态概率取值为1时，熵的值为零。\n",
    "\n",
    "### ID3算法-条件熵和信息增益\n",
    "- 条件熵 $𝐻(𝑌∣𝑋)$ ：\n",
    "\t表示在已知随机变量𝑋的条件下随机变量𝑌的不确定性，定义为给定𝑋条件下𝑌的条件概率分布的熵对𝑋的数学期望:\n",
    "$$H(Y \\mid X)=\\sum_{x} p(x) H(Y \\mid X=x) =-\\sum_{x} p(x) \\sum_{y} p(y \\mid x) \\log p(y \\mid x)$$\n",
    "\n",
    "- 特征𝐴对数据集𝐷的信息增益就是熵$𝐻(𝐷)$与条件熵$𝐻(𝐷|𝐴)$之差:\n",
    "$$𝐻(𝐷)−𝐻(𝐷∣𝐴)$$\n",
    "\n",
    "\t表示已知特征𝐴的信息而使得数据集𝐷的信息不确定减少的程度。信息增益越大的特征代表其具有更强的分类能力，所以我们就要**选择能够使数据的不确定程度减少最多的特征**，也就是信息增益最大的特征。\n",
    "\n",
    "### ID3算法-停止条件\n",
    "- 决策树的生成:\n",
    "\n",
    "\t从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "    \n",
    "    \n",
    "- 迭代停止条件：\n",
    "  1. 当前结点所有样本都属于同⼀类别；\n",
    "  2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "  3. 达到最大树深；\n",
    "  4. 达到叶子结点的最小样本数；\n",
    "\n",
    "### ID3算法举例\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/10/23/p7gSQeYGnoBCd2i.png\" style=\"zoom:64%\" />\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\operatorname{Info}^{\\text {In }}(D)=-\\frac{9}{14} \\log _{2}\\left(\\frac{9}{14}\\right)-\\frac{5}{14} \\log _{2}\\left(\\frac{5}{14}\\right)=0.940 \\\\\n",
    "\\operatorname{Infoage~}(D)=\\frac{5}{14} \\times\\left(-\\frac{2}{5} \\times \\log _{2} \\frac{2}{5}-\\frac{3}{5} \\times \\log _{2} \\frac{3}{5}\\right)+\\frac{4}{14} \\times\\left(-\\frac{4}{4} \\times \\log _{2} \\frac{4}{4}-\\frac{0}{4} \\times \\log _{2} \\frac{0}{4}\\right) \n",
    "+\\frac{5}{14} \\times\\left(-\\frac{2}{5} \\times \\log _{2} \\frac{2}{5}-\\frac{3}{5} \\times \\log _{2} \\frac{3}{5}\\right)=0.694 \\\\\n",
    "\\text { Gain }(\\text { age })=\\operatorname{Info}(D)-\\operatorname{InfO}_{\\text {age }}(D) =0.940-0.694=0.246\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<img src=\"https://s2.loli.net/2022/10/23/1zAnHWKRgQ9FaJV.png\" style=\"zoom:72%\" />\n",
    "类似地，\n",
    "Gain(income)=0.029    \n",
    "Gain(student)=0.151    \n",
    "Gain(credit_rating)=0.048\n",
    "\n",
    "所以，选择age作为第一个根节点\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160ac51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建 ID3 决策树模型\n",
    "dt_classifier = DecisionTreeClassifier(criterion='entropy')\n",
    "dt_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13874a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类精度：0.70\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# 输出分类精度\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"分类精度：{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6875744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, value=None, result=None):\n",
    "        self.feature = feature  # 特征列名\n",
    "        self.value = value  # 特征取值\n",
    "        self.result = result  # 结果标签\n",
    "        self.children = {}  # 子节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a88d86",
   "metadata": {},
   "source": [
    "X是⼀个取值为有限个的离散随机变量，\n",
    "$$ H(X)=-\\sum_{i=1}^{n} p\\left(x_{i}\\right) \\log p\\left(x_{i}\\right)$$ \n",
    "$𝐻(𝑋)$就被称作随机变量𝑋的熵，它表示随机变量不确定的度量。熵取值越大，随机变量不确定性越大。当随机变量为均匀分布时，熵最大。当某一状态概率取值为1时，熵的值为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b0e5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算熵\n",
    "def entropy(y):\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)# 获取类别标签及其对应的出现次数\n",
    "    probabilities = counts / len(y)# 计算每个类别的概率\n",
    "    return -np.sum(probabilities * np.log2(probabilities + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f4014",
   "metadata": {},
   "source": [
    "- 条件熵 $𝐻(𝑌∣𝑋)$ ：\n",
    "\t表示在已知随机变量𝑋的条件下随机变量𝑌的不确定性，定义为给定𝑋条件下𝑌的条件概率分布的熵对𝑋的数学期望:\n",
    "$$H(Y \\mid X)=\\sum_{x} p(x) H(Y \\mid X=x) =-\\sum_{x} p(x) \\sum_{y} p(y \\mid x) \\log p(y \\mid x)$$\n",
    "\n",
    "- 特征𝐴对数据集𝐷的信息增益就是熵$𝐻(𝐷)$与条件熵$𝐻(𝐷|𝐴)$之差:\n",
    "$$𝐻(𝐷)−𝐻(𝐷∣𝐴)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c177c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息增益\n",
    "def information_gain(X, y, feature):\n",
    "    # 计算基础熵（即未进行任何特征划分时的熵）\n",
    "    base_entropy = entropy(y)\n",
    "    # 获取特征列的唯一值\n",
    "    unique_values = X[feature].unique()\n",
    "    # 初始化加权熵\n",
    "    weighted_entropy = 0\n",
    "    # 遍历特征的每个取值\n",
    "    for value in unique_values:\n",
    "         # 获取特征X取值为value的样本的索引\n",
    "        subset_indices = X[X[feature] == value].index\n",
    "          # 计算在特征 X 取值为 value 的条件下目标变量 y 的熵\n",
    "        subset_entropy = entropy(y.loc[subset_indices])\n",
    "         # 将条件熵加权累加\n",
    "        weighted_entropy += len(subset_indices) / len(y) * subset_entropy\n",
    " # 计算信息增益\n",
    "    return base_entropy - weighted_entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aafbdf",
   "metadata": {},
   "source": [
    "- 决策树的生成:\n",
    "\n",
    "\t从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "    \n",
    "    \n",
    "- 迭代停止条件：\n",
    "  1. 当前结点所有样本都属于同⼀类别；\n",
    "  2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "  3. 达到最大树深；\n",
    "  4. 达到叶子结点的最小样本数；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86887023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建ID3决策树\n",
    "def id3(X, y, features):   \n",
    "    # 如果目标变量y的取值只有一个，返回一个叶子节点，表示该子树的预测结果为该唯一取值\n",
    "    if len(set(y)) == 1:\n",
    "        return Node(result=y.iloc[0])\n",
    "\n",
    "    # 如果特征集为空，表示所有特征都已经使用过。没有剩余属性可用来进一步划分样本，停止迭代\n",
    "    # 返回一个叶子节点，表示该子树的预测结果为目标变量y中出现次数最多的取值\n",
    "    if len(features) == 0:\n",
    "        return Node(result=y.mode().iloc[0])\n",
    "\n",
    "    # 选择信息增益最大的特征作为当前节点的划分特征\n",
    "    best_feature = max(features, key=lambda feature: information_gain(X, y, feature))\n",
    "    root = Node(feature=best_feature)\n",
    "\n",
    "     # 对于划分特征的每个取值，递归构建子树\n",
    "    for value in X[best_feature].unique():\n",
    "        # 找到划分特征取值为value的样本索引\n",
    "        subset_indices = X[X[best_feature] == value].index\n",
    "        # 递归构建子树\n",
    "        child_node = id3(X.loc[subset_indices], y.loc[subset_indices], features - {best_feature})\n",
    "         # 将子树加入当前节点的子节点字典\n",
    "        root.children[value] = child_node\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52df40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测单个样本\n",
    "def predict(node, sample):   \n",
    "    if node.result is not None:\n",
    "        return node.result\n",
    "\n",
    "    value = sample[node.feature]\n",
    "    if value in node.children:\n",
    "        return predict(node.children[value], sample)\n",
    "    else:\n",
    "        # 如果测试集中出现了训练集中未见过的取值，返回一个默认值\n",
    "        return node.children[list(node.children.keys())[0]].result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c59f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量预测\n",
    "def batch_predict(node, X):    \n",
    "    return X.apply(lambda sample: predict(node, sample), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb3b2a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征和标签\n",
    "X_train = watermelon_train1_data.drop('好瓜', axis=1)\n",
    "y_train = watermelon_train1_data['好瓜']\n",
    "\n",
    "X_test = watermelon_test1_data.drop('好瓜', axis=1)\n",
    "y_test = watermelon_test1_data['好瓜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b611ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征集合\n",
    "features = set(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd9b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建ID3决策树\n",
    "root = id3(X_train, y_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6bbc629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类精度： 0.70\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行预测\n",
    "y_pred = batch_predict(root, X_test)\n",
    "\n",
    "# 输出分类精度\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"分类精度：{accuracy: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11877b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d14b3d2b",
   "metadata": {},
   "source": [
    "### 中级要求\n",
    "a)  对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "\n",
    "b)\t对测试集Watermelon-test2进行预测，输出分类精度；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4c777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练集和测试集数据\n",
    "watermelon_train2_data = pd.read_csv(r\"C:/Users/LENOVO/Desktop/ML-6/Watermelon-train2.csv\", encoding='gbk')  \n",
    "watermelon_test2_data = pd.read_csv(r\"C:/Users/LENOVO/Desktop/ML-6/Watermelon-test2.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9756eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分类标签转换为数值表示，是->1，否->0\n",
    "watermelon_train2_data['好瓜'] = watermelon_train2_data['好瓜'].map({'是': 1, '否': 0})\n",
    "watermelon_test2_data['好瓜'] = watermelon_test2_data['好瓜'].map({'是': 1, '否': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6638cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将类别型特征转换为数字\n",
    "le = LabelEncoder()\n",
    "for column in watermelon_train2_data.columns:\n",
    "    if watermelon_train2_data[column].dtype == 'object':\n",
    "        watermelon_train2_data[column] = le.fit_transform(watermelon_train2_data[column])\n",
    "        \n",
    "for column in watermelon_test2_data.columns:\n",
    "    if watermelon_test2_data[column].dtype == 'object':\n",
    "        watermelon_test2_data[column] = le.fit_transform(watermelon_test2_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c48e2627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行独热编码\n",
    "# watermelon_train2_data = pd.get_dummies(watermelon_train2_data, columns=['色泽', '根蒂', '敲声', '纹理', '密度'])\n",
    "# watermelon_test2_data = pd.get_dummies(watermelon_test2_data, columns=['色泽', '根蒂', '敲声', '纹理','密度'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8540ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练集特征和标签\n",
    "X_train = watermelon_train2_data.drop(columns=['好瓜'])\n",
    "y_train = watermelon_train2_data ['好瓜']\n",
    "\n",
    "# 分割测试集特征和标签\n",
    "X_test = watermelon_test2_data.drop(columns=['好瓜'])\n",
    "y_test = watermelon_test2_data['好瓜']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd7383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1bd22c3",
   "metadata": {},
   "source": [
    "### 决策树的划分\n",
    "- 决策树主要分为三种：\n",
    "\tID3，C4.5和CART，它们分别对应的**特征选择准则**是信息增益（ID3），信息增益比（C4.5）和基尼指数（CART）。\n",
    "\t它们决定当前选择哪个特征进行数据划分，使得样本在当下能够被最大程度的划分。\n",
    "- 对于离散变量，选定**属性**分类即可；\n",
    "- 对于连续变量，需要选定**划分点**。\n",
    "- CART和C4.5支持数据特征为**连续分布**时的处理，能够完成对连续属性的离散化处理，主要通过二元切分的方式来处理连续型变量，这个分裂点的选择原则是使得划分后的子树中的“混乱程度”降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dccfdc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建C4.5决策树\n",
    "c4_5_classifier = DecisionTreeClassifier(criterion='entropy')\n",
    "c4_5_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78ea324b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 Classification Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "# 预测并输出分类精度\n",
    "y_pred_c4_5 = c4_5_classifier.predict(X_test)\n",
    "accuracy_c4_5 = accuracy_score(y_test, y_pred_c4_5)\n",
    "print(f\"C4.5 Classification Accuracy: {accuracy_c4_5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc210241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement c45 (from versions: none)\n",
      "ERROR: No matching distribution found for c45\n"
     ]
    }
   ],
   "source": [
    "pip install c45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7854d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建CART决策树\n",
    "cart_classifier = DecisionTreeClassifier(criterion='gini')\n",
    "cart_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4534042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Classification Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "# 预测并输出分类精度\n",
    "y_pred_cart = cart_classifier.predict(X_test)\n",
    "accuracy_cart = accuracy_score(y_test, y_pred_cart)\n",
    "print(f\"CART Classification Accuracy: {accuracy_cart:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b9a0c",
   "metadata": {},
   "source": [
    "### C4.5算法\n",
    "- C4.5算法与ID3算法相似，其对ID3算法进行了改进。\n",
    "- 信息增益作为划分准则存在的问题：\n",
    "\n",
    "     信息增益偏向于选择取值较多的特征进行划分。⽐如学号这个特征，每个学生都有一个不同的学号，如果根据学号对样本进行分类，则每个学生都属于不同的类别，这样是没有意义的。而C4.5在生成过程中，用**信息增益比**来选择特征，可以校正这个问题。\n",
    "     \n",
    "- 特点\n",
    "  - 能够完成对连续属性的离散化处理\n",
    "  - 能够对不完整数据进行处理\n",
    "  - 需要对数据集进行多次的顺序扫描和排序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0adb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C45DecisionTree:\n",
    "    \n",
    "    \"\"\"\n",
    "    min_samples_split: int, optional (default=2)\n",
    "          节点分裂的最小样本数\n",
    "    max_depth: int or None, optional (default=None)\n",
    "          树的最大深度。如果为None，则节点会一直分裂，直到每个叶子节点的样本数小于min_samples_split     \n",
    "    tree: dict\n",
    "          存储构建好的决策树的字典    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_samples_split=2, max_depth=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    # 计算信息熵\n",
    "    def _calculate_entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    \n",
    "    # 根据训练数据拟合C4.5决策树\n",
    "    def fit(self, X, y):\n",
    "        features = X.columns\n",
    "        data = pd.concat([X, y], axis=1)\n",
    "        self.tree = self._build_tree(data, features)\n",
    "\n",
    "    # 递归构建C4.5决策树\n",
    "    def _build_tree(self, data, features, depth=0):\n",
    "        # 若样本数量小于等于阈值或深度达到最大深度，则返回叶子节点\n",
    "        if len(data) <= self.min_samples_split or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return self._create_leaf_node(data)\n",
    "\n",
    "        # 选择最佳特征和阈值进行分裂\n",
    "        selected_feature, threshold = self._choose_best_feature(data, features)\n",
    "\n",
    "        # 根据选择的特征和阈值进行分裂\n",
    "        left_data = data[data[selected_feature] <= threshold]\n",
    "        right_data = data[data[selected_feature] > threshold]\n",
    "\n",
    "        # 递归构建左右子树\n",
    "        left_subtree = self._build_tree(left_data, features, depth + 1)\n",
    "        right_subtree = self._build_tree(right_data, features, depth + 1)\n",
    "\n",
    "        return {'feature': selected_feature, 'threshold': threshold,\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "   \n",
    "    # 创建叶子节点，包含数据集中出现最频繁的类别\n",
    "    def _create_leaf_node(self, data):\n",
    "        values, counts = np.unique(data.iloc[:, -1], return_counts=True)\n",
    "        return {'class': values[np.argmax(counts)]}\n",
    "    \n",
    "    # 选择最佳特征和阈值进行分裂\n",
    "    def _choose_best_feature(self, data, features):\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_info_gain_ratio = -1\n",
    "\n",
    "        for feature in features:\n",
    "            if data[feature].dtype == 'object':\n",
    "                # 处理离散特征\n",
    "                info_gain_ratio, threshold = self._calculate_info_gain_ratio(data, feature)\n",
    "            else:\n",
    "                # 处理连续特征\n",
    "                info_gain_ratio, threshold = self._calculate_info_gain_ratio_continuous(data, feature)\n",
    "\n",
    "            # 选择信息增益比最大的特征和相应的阈值\n",
    "            if info_gain_ratio > best_info_gain_ratio:\n",
    "                best_info_gain_ratio = info_gain_ratio\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # 计算信息增益比\n",
    "    def _calculate_info_gain_ratio(self, data, feature):\n",
    "\n",
    "        # 计算原始数据的信息熵\n",
    "        original_entropy = self._calculate_entropy(data.iloc[:, -1])\n",
    "\n",
    "        # 计算按当前特征分裂后的条件熵\n",
    "        values, counts = np.unique(data[feature], return_counts=True)\n",
    "        weighted_entropy = 0\n",
    "        for value, count in zip(values, counts):\n",
    "            subset_indices = data[data[feature] == value].index\n",
    "            subset_entropy = self._calculate_entropy(data.loc[subset_indices].iloc[:, -1])\n",
    "            weighted_entropy += count / len(data) * subset_entropy\n",
    "\n",
    "        # 计算信息增益\n",
    "        information_gain = original_entropy - weighted_entropy\n",
    "\n",
    "        # 计算分裂信息\n",
    "        split_info = self._calculate_split_info(data[feature])\n",
    "\n",
    "        # 避免分母为零\n",
    "        if split_info == 0:\n",
    "            return 0, 0\n",
    "\n",
    "        # 计算信息增益比\n",
    "        info_gain_ratio = information_gain / split_info\n",
    "\n",
    "        return info_gain_ratio, None  # 对于离散特征，不需要阈值\n",
    "\n",
    "    # 计算处理连续特征的信息增益比\n",
    "    def _calculate_info_gain_ratio_continuous(self, data, feature):\n",
    "        # 排序并获取唯一值\n",
    "        unique_values = np.unique(data[feature])\n",
    "        unique_values.sort()\n",
    "\n",
    "        best_info_gain_ratio = -1\n",
    "        best_threshold = None\n",
    "\n",
    "        # 遍历每个可能的阈值,计算每个阈值下的信息增益比\n",
    "        for i in range(len(unique_values) - 1):\n",
    "            threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "            info_gain_ratio, _ = self._calculate_info_gain_ratio_continuous_with_threshold(data, feature, threshold)\n",
    "\n",
    "            # 选择最大的信息增益比和相应的阈值\n",
    "            if info_gain_ratio > best_info_gain_ratio:\n",
    "                best_info_gain_ratio = info_gain_ratio\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_info_gain_ratio, best_threshold\n",
    "    \n",
    "    # 计算给定阈值下的信息增益比\n",
    "    def _calculate_info_gain_ratio_continuous_with_threshold(self, data, feature, threshold):\n",
    "        # 分割数据\n",
    "        left_subset = data[data[feature] <= threshold]\n",
    "        right_subset = data[data[feature] > threshold]\n",
    "\n",
    "        # 计算原始数据的信息熵\n",
    "        original_entropy = self._calculate_entropy(data.iloc[:, -1])\n",
    "\n",
    "        # 计算按当前特征分裂后的条件熵\n",
    "        left_weight = len(left_subset) / len(data)\n",
    "        right_weight = len(right_subset) / len(data)\n",
    "\n",
    "        left_subset_entropy = self._calculate_entropy(left_subset.iloc[:, -1])\n",
    "        right_subset_entropy = self._calculate_entropy(right_subset.iloc[:, -1])\n",
    "\n",
    "        weighted_entropy = left_weight * left_subset_entropy + right_weight * right_subset_entropy\n",
    "\n",
    "        # 计算信息增益\n",
    "        information_gain = original_entropy - weighted_entropy\n",
    "\n",
    "        # 计算分裂信息\n",
    "        split_info = self._calculate_split_info_continuous(data[feature], threshold)\n",
    "\n",
    "        # 避免分母为零\n",
    "        if split_info == 0:\n",
    "            return 0, threshold\n",
    "\n",
    "        # 计算信息增益比\n",
    "        info_gain_ratio = information_gain / split_info\n",
    "\n",
    "        return info_gain_ratio, threshold\n",
    "\n",
    "    # 计算分裂信息（离散特征）\n",
    "    def _calculate_split_info(self, x):\n",
    "        _, counts = np.unique(x, return_counts=True)\n",
    "        probabilities = counts / len(x)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "     # 计算分裂信息（连续特征）\n",
    "    def _calculate_split_info_continuous(self, x, threshold):\n",
    "        left_probability = np.sum(x <= threshold) / len(x)\n",
    "        right_probability = 1 - left_probability\n",
    "        return -left_probability * np.log2(left_probability + 1e-10) - right_probability * np.log2(right_probability + 1e-10)\n",
    "\n",
    "    # 新样本的预测逻辑\n",
    "    # 遍历决策树进行预测\n",
    "    def predict(self, X):       \n",
    "        predictions = []\n",
    "        for _, sample in X.iterrows():\n",
    "            predictions.append(self._traverse_tree(sample, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    # 递归遍历决策树，直到叶子节点\n",
    "    def _traverse_tree(self, sample, node):\n",
    "        if 'class' in node:\n",
    "            return node['class']\n",
    "        else:\n",
    "            if sample[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(sample, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(sample, node['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f7c4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c45_tree = C45DecisionTree(min_samples_split=2, max_depth=None)\n",
    "c45_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c57a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 Classification Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "y_pred = c45_tree.predict(X_test)\n",
    "accuracy_c4_5 = accuracy_score(y_test, y_pred)\n",
    "print(f\"C4.5 Classification Accuracy: {accuracy_c4_5:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603fc76",
   "metadata": {},
   "source": [
    "\n",
    "### CART算法\n",
    "- ID3和C4.5虽然在对训练样本集的学习中可以尽可能多的挖掘信息，但其生成的决策树分支较大，规模较大。为了简化决策树的规模，提高生成决策树的效率，就出现了根据**基尼指数**来选择的CART； \n",
    "- 对于给定的样本集合 ，其基尼指数为： $$ {Gini}(D)=1-\\sum_{k=1}^{K}\\left(\\frac{\\left|C_{k}\\right|}{|D|}\\right)^{2} $$\n",
    "   其中$𝐶_𝑘$是𝐷中属于第𝑘类的样本子集，K是类的个数。\n",
    "- 基尼系数的性质与信息熵一样：\n",
    "   度量随机变量的不确定度的大小；基尼指数越⼩表示数据的纯度越高，反之其值越大，样本集合的不确定性也就越大。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "926c15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARTDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    # 构建决策树\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "         # 若达到最大深度或样本数量小于等于阈值，则返回叶子节点\n",
    "        if self.max_depth is not None and depth == self.max_depth or len(X) <= self.min_samples_split:\n",
    "            return self._create_leaf_node(y)\n",
    "\n",
    "         # 选择最佳特征和阈值进行分裂\n",
    "        best_feature, best_threshold = self._choose_best_split(X, y)\n",
    "\n",
    "        # 若无法找到合适的分裂点，则返回叶子节点\n",
    "        if best_feature is None:\n",
    "            return self._create_leaf_node(y)\n",
    "\n",
    "        # 根据选择的特征和阈值进行分裂\n",
    "        left_indices = X[best_feature] <= best_threshold\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        # 递归构建左右子树\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {'feature': best_feature, 'threshold': best_threshold,\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    # 选择最佳特征和阈值进行分裂\n",
    "    def _choose_best_split(self, X, y):\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_mse = float('inf')\n",
    "\n",
    "         # 遍历所有特征\n",
    "        for feature in X.columns:\n",
    "            # 计算每个特征可能的分裂点及其均方误差\n",
    "            thresholds, mse_values = self._calculate_mse_for_feature(X[feature], y)\n",
    "            \n",
    "             # 检查mse_values是否为空\n",
    "            if len(mse_values) == 0:\n",
    "                continue\n",
    "                \n",
    "            min_mse_index = np.argmin(mse_values)\n",
    "            min_mse = mse_values[min_mse_index]\n",
    "\n",
    "            # 更新最佳分裂点\n",
    "            if min_mse < best_mse:\n",
    "                best_mse = min_mse\n",
    "                best_feature = feature\n",
    "                best_threshold = thresholds[min_mse_index]\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # 计算给定特征的每个可能分裂点及其均方误差\n",
    "    def _calculate_mse_for_feature(self, feature, y):\n",
    "        unique_values = np.unique(feature)\n",
    "        thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "\n",
    "        mse_values = []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            left_indices = feature <= threshold\n",
    "            right_indices = ~left_indices\n",
    "\n",
    "            mse = self._calculate_mse(y[left_indices]) + self._calculate_mse(y[right_indices])\n",
    "            mse_values.append(mse)\n",
    "\n",
    "        return thresholds, mse_values\n",
    "\n",
    "    # 计算均方误差\n",
    "    def _calculate_mse(self, values):\n",
    "        if len(values) == 0:\n",
    "            return 0\n",
    "        mean_value = np.mean(values)\n",
    "        return np.mean((values - mean_value) ** 2)\n",
    "\n",
    "    # 创建叶子节点，返回均值作为叶子节点的值\n",
    "    def _create_leaf_node(self, y):\n",
    "        return {'value': np.mean(y)}\n",
    "\n",
    "     # 对输入数据进行预测\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for _, sample in X.iterrows():\n",
    "            predictions.append(self._traverse_tree(sample, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    # 递归遍历决策树，返回叶子节点的值\n",
    "    def _traverse_tree(self, sample, node):\n",
    "        if 'value' in node:\n",
    "            return node['value']\n",
    "        else:\n",
    "            if sample[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(sample, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(sample, node['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47029615",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_tree = CARTDecisionTree(min_samples_split=2, max_depth=None)\n",
    "cart_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50b2ae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Classification Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "y_pred = cart_tree.predict(X_test)\n",
    "accuracy_cart = accuracy_score(y_test, y_pred)\n",
    "print(f\"CART Classification Accuracy: {accuracy_cart:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4ff42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08949992",
   "metadata": {},
   "source": [
    "### 高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814fabb",
   "metadata": {},
   "source": [
    "### 决策树的剪枝\n",
    "- 决策树很容易出现**过拟合现象**。原因在于学习时完全考虑的是如何提⾼对训练数据的正确分类从⽽构建出过于复杂的决策树。\n",
    "- 解决这个问题的方法称为**剪枝**，即对已生成的树进行简化。具体地，就是从已生成的树上裁剪掉⼀些子树或叶节点，并将其根节点或父节点作为新的叶节点。 \n",
    "- 决策树的剪枝基本策略有**预剪枝 (Pre-Pruning)** 和 **后剪枝 (Post-Pruning)**\n",
    "   - **预剪枝**：是根据⼀些原则**极早的停止树增长**，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的幅度小于用户指定的幅度等。 \n",
    "   - **后剪枝**：是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点。是在生成决策树之后**自底向上**的对树中所有的非叶结点进⾏逐一考察 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "913493b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedCARTDecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=None, alpha=0.1):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.alpha = alpha\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        features = X.columns\n",
    "        data = pd.concat([X, y], axis=1)\n",
    "        self.tree = self._build_tree(data, features)\n",
    "\n",
    "    def _build_tree(self, data, features, depth=0):\n",
    "        if len(data) <= self.min_samples_split or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return self._create_leaf_node(data)\n",
    "\n",
    "        selected_feature, threshold = self._choose_best_split(data, features)\n",
    "\n",
    "        left_data = data[data[selected_feature] <= threshold]\n",
    "        right_data = data[data[selected_feature] > threshold]\n",
    "\n",
    "        validation_accuracy_before_split = self._calculate_accuracy(left_data, right_data)\n",
    "\n",
    "        left_leaf = self._create_leaf_node(left_data)\n",
    "        right_leaf = self._create_leaf_node(right_data)\n",
    "\n",
    "        validation_accuracy_after_split = self._calculate_accuracy(left_leaf, right_leaf)\n",
    "\n",
    "        if validation_accuracy_after_split - validation_accuracy_before_split < self.alpha:\n",
    "            return self._create_leaf_node(data)\n",
    "\n",
    "        left_subtree = self._build_tree(left_data, features, depth + 1)\n",
    "        right_subtree = self._build_tree(right_data, features, depth + 1)\n",
    "\n",
    "        return {'feature': selected_feature, 'threshold': threshold,\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    def _choose_best_split(self, data, features):\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_mse = float('inf')\n",
    "\n",
    "        for feature in features:\n",
    "            thresholds, mse_values = self._calculate_mse_for_feature(data[feature], data.iloc[:, -1])\n",
    "            min_mse_index = np.argmin(mse_values)\n",
    "            min_mse = mse_values[min_mse_index]\n",
    "\n",
    "            if min_mse < best_mse:\n",
    "                best_mse = min_mse\n",
    "                best_feature = feature\n",
    "                best_threshold = thresholds[min_mse_index]\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _calculate_mse_for_feature(self, feature, y):\n",
    "        thresholds = np.unique(feature)\n",
    "        thresholds.sort()\n",
    "\n",
    "        mse_values = []\n",
    "        for threshold in thresholds:\n",
    "            left_indices = feature <= threshold\n",
    "            right_indices = ~left_indices\n",
    "            mse = self._calculate_mse(y[left_indices]) + self._calculate_mse(y[right_indices])\n",
    "            mse_values.append(mse)\n",
    "\n",
    "        return thresholds, mse_values\n",
    "\n",
    "    def _calculate_mse(self, values):\n",
    "        if len(values) == 0:\n",
    "            return 0\n",
    "        mean_value = np.mean(values)\n",
    "        return np.mean((values - mean_value) ** 2)\n",
    "\n",
    "    def _create_leaf_node(self, data):\n",
    "        values, counts = np.unique(data.iloc[:, -1], return_counts=True)\n",
    "        return {'class': values[np.argmax(counts)]}\n",
    "\n",
    "    def _calculate_accuracy(self, left_data, right_data):\n",
    "        return len(left_data) / (len(left_data) + len(right_data))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for _, sample in X.iterrows():\n",
    "            predictions.append(self._traverse_tree(sample, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _traverse_tree(self, sample, node):\n",
    "        if 'class' in node:\n",
    "            return node['class']\n",
    "        else:\n",
    "            if sample[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(sample, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(sample, node['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc4d9a",
   "metadata": {},
   "source": [
    "计算分裂前左右子树的准确率，然后创建左右子树的叶子节点，再计算分裂后的准确率。如果分裂后的准确率减去分裂前的准确率小于 self.alpha，则选择不进行分裂，而是直接返回一个包含整个数据的叶子节点，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "00c6eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_cart_tree = PrunedCARTDecisionTree(min_samples_split=2, max_depth=None, alpha=0.1)\n",
    "pruned_cart_tree.fit(X_train, y_train)\n",
    "\n",
    "# tree = C45DecisionTree(min_samples_split=2, max_depth=None)\n",
    "# tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8494172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned CART Classification Accuracy: 0.40\n"
     ]
    }
   ],
   "source": [
    "# 预测并输出分类精度\n",
    "y_pred = pruned_cart_tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Pruned CART Classification Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05902b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "68e85e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedC45DecisionTree:\n",
    "    \n",
    "    \"\"\"\n",
    "    min_samples_split: int, optional (default=2)\n",
    "          节点分裂的最小样本数\n",
    "    max_depth: int or None, optional (default=None)\n",
    "          树的最大深度。如果为None，则节点会一直分裂，直到每个叶子节点的样本数小于min_samples_split     \n",
    "    tree: dict\n",
    "          存储构建好的决策树的字典    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_samples_split=2, max_depth=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    # 计算信息熵\n",
    "    def _calculate_entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    \n",
    "    # 根据训练数据拟合C4.5决策树\n",
    "    def fit(self, X, y, n_folds=5):\n",
    "        features = X.columns\n",
    "        data = pd.concat([X, y], axis=1)\n",
    "\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_index, val_index in kf.split(data):\n",
    "            train_data, val_data = data.iloc[train_index], data.iloc[val_index]\n",
    "            \n",
    "            self.tree = self._build_tree(train_data, features, validation_data=val_data)\n",
    "            \n",
    "            # 检查在剪枝后 self.tree 是否为 None\n",
    "            if self.tree is not None:\n",
    "                # 如果 self.tree 不是 None，则检查剪枝后的准确率\n",
    "                pruned_tree = self._prune_tree(self.tree, val_data)\n",
    "                if pruned_tree is not None:\n",
    "                    self.tree = pruned_tree\n",
    "\n",
    "    # 递归构建C4.5决策树\n",
    "    def _build_tree(self, data, features, depth=0, validation_data=None):\n",
    "        # 若样本数量小于等于阈值或深度达到最大深度，则返回叶子节点\n",
    "        if len(data) <= self.min_samples_split or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return self._create_leaf_node(data)\n",
    "\n",
    "        # 选择最佳特征和阈值进行分裂\n",
    "        selected_feature, threshold = self._choose_best_feature(data, features)\n",
    "\n",
    "        # 根据选择的特征和阈值进行分裂\n",
    "        left_data = data[data[selected_feature] <= threshold]\n",
    "        right_data = data[data[selected_feature] > threshold]\n",
    "\n",
    "        # 递归构建左右子树\n",
    "        left_subtree = self._build_tree(left_data, features, depth + 1)\n",
    "        right_subtree = self._build_tree(right_data, features, depth + 1)\n",
    "\n",
    "        # 构建完整的树\n",
    "        self.tree = {'feature': selected_feature, 'threshold': threshold, 'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "         # 进行后剪枝\n",
    "        if validation_data is not None:\n",
    "            pruned_tree = self._prune_tree(self.tree, validation_data)\n",
    "            if pruned_tree is not None:\n",
    "                self.tree = pruned_tree\n",
    " \n",
    "        return self.tree\n",
    "   \n",
    "    def _prune_tree(self, tree, validation_data):\n",
    "        if 'left' in tree and 'right' in tree:\n",
    "            left_subtree = tree['left']\n",
    "            right_subtree = tree['right']\n",
    "\n",
    "            if 'class' not in left_subtree and 'class' not in right_subtree:\n",
    "                # 如果左右子树都不是叶子节点，则递归剪枝\n",
    "                tree['left'] = self._prune_tree(left_subtree, validation_data)\n",
    "                tree['right'] = self._prune_tree(right_subtree, validation_data)\n",
    "\n",
    "                # 如果剪枝后的准确率更高，则合并为叶子节点\n",
    "                before_pruning_accuracy = self._calculate_accuracy(validation_data, tree)\n",
    "                tree['class'] = self._majority_class(validation_data)\n",
    "                after_pruning_accuracy = self._calculate_accuracy(validation_data, tree)\n",
    "\n",
    "                if after_pruning_accuracy >= before_pruning_accuracy:\n",
    "                    # 如果准确率提高或保持不变，则返回叶子节点\n",
    "                    return {'class': tree['class']}\n",
    "\n",
    "        return tree\n",
    "    \n",
    "    def _calculate_accuracy(self, data, tree):\n",
    "        predictions = self.predict(data.iloc[:, :-1])\n",
    "        actual_labels = data.iloc[:, -1].to_numpy()\n",
    "        accuracy = np.mean(predictions == actual_labels)\n",
    "        return accuracy\n",
    "    \n",
    "    def _majority_class(self, data):\n",
    "        values, counts = np.unique(data.iloc[:, -1], return_counts=True)\n",
    "        return values[np.argmax(counts)]\n",
    "    \n",
    "    # 创建叶子节点，包含数据集中出现最频繁的类别\n",
    "    def _create_leaf_node(self, data):\n",
    "        values, counts = np.unique(data.iloc[:, -1], return_counts=True)\n",
    "        return {'class': values[np.argmax(counts)]}\n",
    "    \n",
    "    # 选择最佳特征和阈值进行分裂\n",
    "    def _choose_best_feature(self, data, features):\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_info_gain_ratio = -1\n",
    "\n",
    "        for feature in features:\n",
    "            if data[feature].dtype == 'object':\n",
    "                # 处理离散特征\n",
    "                info_gain_ratio, threshold = self._calculate_info_gain_ratio(data, feature)\n",
    "            else:\n",
    "                # 处理连续特征\n",
    "                info_gain_ratio, threshold = self._calculate_info_gain_ratio_continuous(data, feature)\n",
    "\n",
    "            # 选择信息增益比最大的特征和相应的阈值\n",
    "            if info_gain_ratio > best_info_gain_ratio:\n",
    "                best_info_gain_ratio = info_gain_ratio\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # 计算信息增益比\n",
    "    def _calculate_info_gain_ratio(self, data, feature):\n",
    "\n",
    "        # 计算原始数据的信息熵\n",
    "        original_entropy = self._calculate_entropy(data.iloc[:, -1])\n",
    "\n",
    "        # 计算按当前特征分裂后的条件熵\n",
    "        values, counts = np.unique(data[feature], return_counts=True)\n",
    "        weighted_entropy = 0\n",
    "        for value, count in zip(values, counts):\n",
    "            subset_indices = data[data[feature] == value].index\n",
    "            subset_entropy = self._calculate_entropy(data.loc[subset_indices].iloc[:, -1])\n",
    "            weighted_entropy += count / len(data) * subset_entropy\n",
    "\n",
    "        # 计算信息增益\n",
    "        information_gain = original_entropy - weighted_entropy\n",
    "\n",
    "        # 计算分裂信息\n",
    "        split_info = self._calculate_split_info(data[feature])\n",
    "\n",
    "        # 避免分母为零\n",
    "        if split_info == 0:\n",
    "            return 0, 0\n",
    "\n",
    "        # 计算信息增益比\n",
    "        info_gain_ratio = information_gain / split_info\n",
    "\n",
    "        return info_gain_ratio, None  # 对于离散特征，不需要阈值\n",
    "\n",
    "    # 计算处理连续特征的信息增益比\n",
    "    def _calculate_info_gain_ratio_continuous(self, data, feature):\n",
    "        # 排序并获取唯一值\n",
    "        unique_values = np.unique(data[feature])\n",
    "        unique_values.sort()\n",
    "\n",
    "        best_info_gain_ratio = -1\n",
    "        best_threshold = None\n",
    "\n",
    "        # 遍历每个可能的阈值,计算每个阈值下的信息增益比\n",
    "        for i in range(len(unique_values) - 1):\n",
    "            threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "            info_gain_ratio, _ = self._calculate_info_gain_ratio_continuous_with_threshold(data, feature, threshold)\n",
    "\n",
    "            # 选择最大的信息增益比和相应的阈值\n",
    "            if info_gain_ratio > best_info_gain_ratio:\n",
    "                best_info_gain_ratio = info_gain_ratio\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_info_gain_ratio, best_threshold\n",
    "    \n",
    "    # 计算给定阈值下的信息增益比\n",
    "    def _calculate_info_gain_ratio_continuous_with_threshold(self, data, feature, threshold):\n",
    "        # 分割数据\n",
    "        left_subset = data[data[feature] <= threshold]\n",
    "        right_subset = data[data[feature] > threshold]\n",
    "\n",
    "        # 计算原始数据的信息熵\n",
    "        original_entropy = self._calculate_entropy(data.iloc[:, -1])\n",
    "\n",
    "        # 计算按当前特征分裂后的条件熵\n",
    "        left_weight = len(left_subset) / len(data)\n",
    "        right_weight = len(right_subset) / len(data)\n",
    "\n",
    "        left_subset_entropy = self._calculate_entropy(left_subset.iloc[:, -1])\n",
    "        right_subset_entropy = self._calculate_entropy(right_subset.iloc[:, -1])\n",
    "\n",
    "        weighted_entropy = left_weight * left_subset_entropy + right_weight * right_subset_entropy\n",
    "\n",
    "        # 计算信息增益\n",
    "        information_gain = original_entropy - weighted_entropy\n",
    "\n",
    "        # 计算分裂信息\n",
    "        split_info = self._calculate_split_info_continuous(data[feature], threshold)\n",
    "\n",
    "        # 避免分母为零\n",
    "        if split_info == 0:\n",
    "            return 0, threshold\n",
    "\n",
    "        # 计算信息增益比\n",
    "        info_gain_ratio = information_gain / split_info\n",
    "\n",
    "        return info_gain_ratio, threshold\n",
    "\n",
    "    # 计算分裂信息（离散特征）\n",
    "    def _calculate_split_info(self, x):\n",
    "        _, counts = np.unique(x, return_counts=True)\n",
    "        probabilities = counts / len(x)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "     # 计算分裂信息（连续特征）\n",
    "    def _calculate_split_info_continuous(self, x, threshold):\n",
    "        left_probability = np.sum(x <= threshold) / len(x)\n",
    "        right_probability = 1 - left_probability\n",
    "        return -left_probability * np.log2(left_probability + 1e-10) - right_probability * np.log2(right_probability + 1e-10)\n",
    "\n",
    "    # 新样本的预测逻辑\n",
    "    # 遍历决策树进行预测\n",
    "    def predict(self, X):       \n",
    "        predictions = []\n",
    "        for _, sample in X.iterrows():\n",
    "            predictions.append(self._traverse_tree(sample, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    # 递归遍历决策树，直到叶子节点\n",
    "    def _traverse_tree(self, sample, node):\n",
    "        if 'class' in node:\n",
    "            return node['class']\n",
    "        else:\n",
    "            if sample[node['feature']] <= node['threshold']:\n",
    "                return self._traverse_tree(sample, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(sample, node['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f0cc8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_c45_tree = PrunedC45DecisionTree(min_samples_split=2, max_depth=None)\n",
    "pruned_c45_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d05ecbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 Classification Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "y_pred = pruned_c45_tree.predict(X_test)\n",
    "accuracy_pruned_c45 = accuracy_score(y_test, y_pred)\n",
    "print(f\"C4.5 Classification Accuracy: {accuracy_pruned_c45:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c961d",
   "metadata": {},
   "source": [
    "### 决策树的剪枝 \n",
    "- 决策树的剪枝基本策略有**预剪枝 (Pre-Pruning)** 和 **后剪枝 (Post-Pruning)**\n",
    "   - **预剪枝**：是根据⼀些原则**极早的停止树增长**，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的幅度小于用户指定的幅度等。 \n",
    "   - **后剪枝**：是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点。是在生成决策树之后**自底向上**的对树中所有的非叶结点进⾏逐一考察 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be509445",
   "metadata": {},
   "source": [
    "预剪枝\n",
    "- 预剪枝能够减少决策树的规模，因为它在构建树的过程中就进行了剪枝。这可以减少模型的复杂性，降低过拟合的风险.\n",
    "- 预剪枝有助于防止决策树在训练数据上过分拟合，但过早停止划分可能导致模型无法学习复杂的模式，增加了欠拟合的风险\n",
    "\n",
    "后剪枝\n",
    "- 后剪枝在构建完整个树后进行，相比于预剪枝，它更灵活，可以更好地适应数据的复杂性。后剪枝通过剪枝决策树的一部分，有助于提高模型的泛化性能\n",
    "- 通过减少树的复杂性，有助于防止过拟合。它可以根据验证集或交叉验证的性能来确定哪些节点需要剪枝\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a3d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39964cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0072cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae33af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691933d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a1522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460be7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b05de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3a133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f93544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d125e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5dfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c4e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2a156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841bc841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6fcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab7547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fb847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a841b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
